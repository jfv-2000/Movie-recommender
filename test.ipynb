{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "ratings = spark.read.csv(\"data/ratings.csv\", header=True, inferSchema=True)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2], 123)\n",
    "als = ALS(maxIter=5, rank=70, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\")\n",
    "als.setSeed(123)\n",
    "model = als.fit(training)\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "\n",
    "# Calculate the sparsity of the user-item rating matrix\n",
    "# num_ratings = df.select(\"rating\").count()\n",
    "# num_users = df.select(\"userId\").distinct().count()\n",
    "# num_items = df.select(\"movieId\").distinct().count()\n",
    "# num_elements = num_users * num_items\n",
    "# num_non_zero = df.select(\"rating\").na.drop().count()\n",
    "# sparsity = (num_non_zero / num_elements)*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Set the minimum number of ratings per user and per item\n",
    "# min_ratings_user = 20\n",
    "# min_ratings_item = 10\n",
    "\n",
    "# # Filter out users with fewer than min_ratings_user ratings\n",
    "# user_counts = df.groupBy(\"userId\").count().filter(col(\"count\") >= min_ratings_user)\n",
    "# df = df.join(user_counts, \"userId\", \"inner\")\n",
    "\n",
    "# # Filter out items with fewer than min_ratings_item ratings\n",
    "# item_counts = df.groupBy(\"movieId\").count().filter(col(\"count\") >= min_ratings_item)\n",
    "# df = df.join(item_counts, \"movieId\", \"inner\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Building Model ---\n",
    "# Compute the item-item similarity matrix\n",
    "# vectorAssembler = VectorAssembler(inputCols=[\"userId\", \"movieId\"], outputCol=\"features\")\n",
    "# df = vectorAssembler.transform(df)\n",
    "# normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2.0)\n",
    "# df = normalizer.transform(df)\n",
    "# brp = BucketedRandomProjectionLSH(inputCol=\"normFeatures\", outputCol=\"hashes\", bucketLength=0.1, numHashTables=10)\n",
    "# model = brp.fit(df)\n",
    "# df = model.transform(df)\n",
    "# similarity_df = model.approxSimilarityJoin(df, df, 1.0, distCol=\"cosineSimilarity\").select(col(\"datasetA.movieId\").alias(\"movieId1\"), col(\"datasetB.movieId\").alias(\"movieId2\"), col(\"cosineSimilarity\"))\n",
    "# # filter to get only the top 10 with the highest similarity\n",
    "# similarity_df = similarity_df.filter(col(\"movieId1\") != col(\"movieId2\")).orderBy(col(\"cosineSimilarity\").desc()).limit(10)\n",
    "# similarity_df.show(10)\n",
    "\n",
    "# # Perform PCA on the similarity matrix\n",
    "# vectorAssembler = VectorAssembler(inputCols=[\"cosineSimilarity\"], outputCol=\"features\")\n",
    "# df = vectorAssembler.transform(similarity_df)\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "# scalerModel = scaler.fit(df)\n",
    "# df = scalerModel.transform(df)\n",
    "# pca = PCA(k=3, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "# pcaModel = pca.fit(df)\n",
    "# df = pcaModel.transform(df)\n",
    "\n",
    "# # Get the top-k similar items for each item\n",
    "# similarity_threshold = 0.8\n",
    "# top_k = 2\n",
    "# similarity_df = df.filter(col(\"pcaFeatures\").getItem(0) >= similarity_threshold)\n",
    "# similarity_df = similarity_df.orderBy(col(\"pcaFeatures\").getItem(0).desc()).limit(top_k)\n",
    "# similarity_df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# take a subset of the data\n",
    "# df = df.sample(0.00038461538, 111)\n",
    "# df.show(100)\n",
    "# (training, test) = df.randomSplit([0.8, 0.2], 123)\n",
    "# training = training.groupBy(\"userId\").pivot(\"movieId\").agg({\"rating\": \"first\"}).na.fill(0)\n",
    "# test = test.groupBy(\"userId\").pivot(\"movieId\").agg({\"rating\": \"first\"}).na.fill(0)\n",
    "# training.show(100)\n",
    "# test.show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
