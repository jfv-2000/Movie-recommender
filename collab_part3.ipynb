{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Recommendations Engine (Part 3/3)\n",
    "\n",
    "> This notebook is part of a series of notebooks that will walk you through the process of building a good collaborative recommendations engine (while also including our mistakes that we did). The series is broken up into three parts. If you haven't already, we would recommend you to read the first two parts of the series before continuing on with this one (as we won't repeat the same explanations).\n",
    "\n",
    "- Part 1: Our Attempt at Building an Item-Item Collaborative Recommendations Engine\n",
    "- Part 2: Fixing our Item-Item Collaborative Recommendations Engine\n",
    "- **Part 3: Improving our Collaborative Recommendations Engine by leveraging other techniques than Item-Item Collaborative Filtering...**\n",
    "\n",
    "## Part 3: Improving our Collaborative Recommendations Engine by leveraging other techniques...\n",
    "In Part 2, we were able to have a working-model, but with non-optimal performance. In this notebook, we want to have a look at other techniques that is used in the industry in order to have a better model.\n",
    "\n",
    "Althought our previous code was using Spark + Parallelizations, our algorithm wasn't optimized for large datasets due to cross-joins and many dataframes manipulations. We realized (in Part 2) that our method isn't frequently used in the industry... **We did a mistake.** It is mostly used for small datasets or for educational purposes. We need to think bigger and use a more scalable/efficient solution such as:\n",
    "\n",
    "- _ANN (Approximate Nearest Neighbors)_ which is a technique that is used to find similar items in a large dataset.\n",
    "- _KNN (K-Nearest Neighbors)_ which is also a technique that is used to find similar items in a large dataset.\n",
    "- _SVD (Singular Value Decomposition)_ which is a matrix factorization technique that is used to find latent factors in a large dataset. \n",
    "- _ALS (Alternating Least Squares)_ which is also a matrix factorization technique that is used to find latent factors in a large dataset.\n",
    "\n",
    "In our case, we believe it makes more sense to use ALS since it is a matrix factorization technique that we learned in class where it is using latent factors. We will test it with item-item and user-item using parameters tuning + biases. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Spark Configuration/Setup\n",
    "Idem to Part 2, we will be using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation/Loading\n",
    "Idem to Part 2, we will be using the same dataset + Same Data Preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Modeling\n",
    "\n",
    "TODO: GRAEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Prediction\n",
    "TODO: GRAEME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Model Evaluation\n",
    "\n",
    "TODO GRAEME: TRY TO USE MULTIPLE EVALUATION TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Performance\n",
    "\n",
    "TODO: Add details on values of RMSE/ETC based on different values of parameters + ensure we have the same seed to have the same ground truth\n",
    "and Explain where is RMSE in scale value and if it's good and if it makes sense compare to netflix prize scale...\n",
    "\n",
    "and discuss about ur improvements with hyperparameters tuning for item-item AND user-user\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>Test 1</th>\n",
    "<th>Test 2</th>\n",
    "<th>Test 3</th>\n",
    "<th>Test 4</th>\n",
    "</tr>\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "\n",
    "| Params Value | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Params Value | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Params Value | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Params Value | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr> </table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: And? What's next?\n",
    "\n",
    "Conclusion of our learning, mistakes, how to improve it, which algo is better etc...\n",
    "\n",
    "our mistakes: had to play with spark parameters to make it work for our memory (had a lot of heap problems), import issues, scaling, moving around df (conversion is heavy - what works with something small might not work), pearson doesnt work well with our type of values...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:15 WARN Utils: Your hostname, Martin-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.53 instead (on interface en0)\n",
      "23/03/23 22:13:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:30 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/03/23 22:13:30 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/03/23 22:13:30 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                        (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:31 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|     1|[{1172, 3.4955099...|\n",
      "|     3|[{27773, 4.371879...|\n",
      "|     5|[{1948, 4.533952}...|\n",
      "|     6|[{83318, 4.673204...|\n",
      "|    12|[{3879, 4.9009957...|\n",
      "|    13|[{88125, 4.13369}...|\n",
      "|    15|[{4302, 4.893448}...|\n",
      "|    16|[{318, 4.9723654}...|\n",
      "|    19|[{83411, 5.161693...|\n",
      "|    20|[{51471, 4.917141...|\n",
      "|    22|[{67504, 4.292516...|\n",
      "|    26|[{65514, 4.481471...|\n",
      "|    27|[{318, 4.6969137}...|\n",
      "|    28|[{1172, 5.132921}...|\n",
      "|    31|[{83318, 4.783527...|\n",
      "|    34|[{83318, 4.953843...|\n",
      "|    37|[{68073, 5.136165...|\n",
      "|    40|[{67504, 5.185649...|\n",
      "|    41|[{83359, 4.917856...|\n",
      "|    43|[{54328, 4.326411...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Best hyperparameters:\n",
      "  maxIter: 15\n",
      "  regParam: 0.10\n",
      "  rank: 50\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Item-Item Recommender System\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data as a Spark DataFrame\n",
    "csv_file_path = \"data/ratings_small.csv\"\n",
    "data = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Split the data into a training set and a testing set\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the ALS algorithm for collaborative filtering\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "# Set up the hyperparameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter, [5, 10, 15]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(als.rank, [10, 20, 50]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator for RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Set up cross-validation\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Perform cross-validation and get the best model\n",
    "cv_model = cross_validator.fit(train_data)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model by calculating the RMSE (Root Mean Squared Error)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.4f}\".format(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "user_recs = best_model.recommendForAllUsers(10)\n",
    "user_recs.show()\n",
    "\n",
    "# After fitting the cross-validator and obtaining the best model\n",
    "print(\"Best hyperparameters:\")\n",
    "print(\"  maxIter: {}\".format(best_model._java_obj.parent().getMaxIter()))\n",
    "print(\"  regParam: {:.2f}\".format(best_model._java_obj.parent().getRegParam()))\n",
    "print(\"  rank: {}\".format(best_model.rank))\n",
    "\n",
    "# Best hyperparameters (after running the code for 30min...):\n",
    "#   maxIter: 15\n",
    "#   regParam: 0.10\n",
    "#   rank: 50\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
