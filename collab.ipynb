{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pyspark.pandas as ps\n",
    "#------------------do not touch------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import  Vectors\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum, count as spark_count\n",
    "#------------------------------------------------\n",
    "\n",
    "# TODO: Figure out why we always need to restart the kernel... probably due to an import...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 03:27:43 WARN Utils: Your hostname, Martin-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.53 instead (on interface en0)\n",
      "23/03/24 03:27:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 03:27:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.co_sym(x, y)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Initialization + Configuration of UDFs\n",
    "\n",
    "#Initialize a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"recommenderTest\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"20000\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "def pearson_average(v):\n",
    "    sum_nonzero = sum(v)\n",
    "    count_nonzero = len([e for e in v if e != 0])\n",
    "    mean = sum_nonzero / count_nonzero\n",
    "    v2 = [e - mean if e != 0 else 0 for e in v] # now subtract the mean from each non zero element\n",
    "    return Vectors.dense(v2)\n",
    "\n",
    "def co_sym (x, y):\n",
    "    pearson1 = pearson_average(x)\n",
    "    pearson2 = pearson_average(y)\n",
    "    return float(pearson1.dot(pearson2)/(Vectors.norm(pearson1,2)*Vectors.norm(pearson2,2)))\n",
    "\n",
    "dot_udf = udf(co_sym, DoubleType())\n",
    "spark.udf.register(\"dot_udf\", dot_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|userId|rating|\n",
      "+-------+------+------+\n",
      "|      1|     1|     1|\n",
      "|      1|     3|     3|\n",
      "|      1|     6|     5|\n",
      "|      1|     9|     5|\n",
      "|      1|    11|     4|\n",
      "|      2|     3|     5|\n",
      "|      2|     4|     4|\n",
      "|      2|     7|     4|\n",
      "|      2|    10|     2|\n",
      "|      2|    11|     1|\n",
      "+-------+------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "similarity_matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|movieId|movieId_1|similarity|\n",
      "+-------+---------+----------+\n",
      "|      4|        3|-0.6239...|\n",
      "|      4|        6|-0.2353...|\n",
      "|      4|        5|0.45873...|\n",
      "|      4|        1|-0.1024...|\n",
      "|      4|        2|0.46800...|\n",
      "|      3|        4|-0.6239...|\n",
      "|      3|        6|0.50636...|\n",
      "|      3|        5|-0.2842...|\n",
      "|      3|        1|0.41403...|\n",
      "|      3|        2|-0.5262...|\n",
      "+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File Loading\n",
    "\n",
    "df = spark.read.csv(\"data/ratings_tiny.csv\", header=True, inferSchema=True)\n",
    "# File FORMAT: userId, movieId, rating, timestamp\n",
    "df = df.drop(\"timestamp\")\n",
    "\n",
    "# ==============DF Initialization (to be used later...)==============\n",
    "\n",
    "df_user_movie_rating = df\n",
    "df_user_movie_rating.show(10, 10)\n",
    "\n",
    "# ==============Similarity Matrix==============\n",
    "# Group by movieId and pivot the userId column\n",
    "df = df.groupBy(\"movieId\").pivot(\"userId\").agg({\"rating\": \"first\"}).fillna(0)\n",
    "\n",
    "# Assemble the columns into a vector column\n",
    "assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select('movieId', 'features')\n",
    "df_vector = df_vector.repartition(10)\n",
    "\n",
    "# Compute the similarity matrix using the dot product of normalized vectors\n",
    "similarity_matrix = df_vector.alias(\"a\").crossJoin(df_vector.alias(\"b\")) \\\n",
    "    .where(\"a.movieId != b.movieId\") \\\n",
    "    .selectExpr(\"a.movieId as movieId\", \"b.movieId as movieId_1\",\n",
    "                \"dot_udf(a.features, b.features) as similarity\")\n",
    "\n",
    "#just show first 10 rows and 10 columns\n",
    "print(\"similarity_matrix\")\n",
    "similarity_matrix.show(10, 10)\n",
    "\n",
    "# Pivot the similarity matrix\n",
    "\n",
    "# similarity_matrix = similarity_matrix.groupBy(\"movieId\").pivot(\"movieId_1\").agg({\"similarity\": \"first\"}).fillna(0)\n",
    "# similarity_matrix.select(similarity_matrix.columns[:10]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+--------------------+\n",
      "|movieId|userId|rating|movieId_1|          similarity|\n",
      "+-------+------+------+---------+--------------------+\n",
      "|      1|     1|     1|        2|-0.17854212213729673|\n",
      "|      1|     1|     1|        5|-0.30895719032666236|\n",
      "|      1|     1|     1|        6|  0.5870395085642741|\n",
      "|      1|     1|     1|        3| 0.41403933560541256|\n",
      "|      1|     1|     1|        4|-0.10245014273309601|\n",
      "|      1|     3|     3|        2|-0.17854212213729673|\n",
      "|      1|     3|     3|        5|-0.30895719032666236|\n",
      "|      1|     3|     3|        6|  0.5870395085642741|\n",
      "|      1|     3|     3|        3| 0.41403933560541256|\n",
      "|      1|     3|     3|        4|-0.10245014273309601|\n",
      "+-------+------+------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "predicted_rating1 2.5864068884261053\n",
      "predicted_rating2 1.720103231368863\n"
     ]
    }
   ],
   "source": [
    "# ==============Prediction==============\n",
    "\n",
    "# predict ratings for user_movie_ratings df\n",
    "df_user_movie_rating = df_user_movie_rating.join(similarity_matrix, df_user_movie_rating.movieId == similarity_matrix.movieId, how='left').drop(similarity_matrix.movieId)\n",
    "df_user_movie_rating = df_user_movie_rating.withColumnRenamed(\"similarity.movieId\", \"movie2\")\n",
    "\n",
    "\n",
    "\n",
    "#------------------do not touch------------------\n",
    "# TODO: figure out this... probably the reason why we need to restart kernel.. must be messing up with other imports...\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import array, col, sort_array\n",
    "#------------------------------------------------\n",
    "\n",
    "\n",
    "def predict_user_rating(user_id, movie_id, similarity_matrix):\n",
    "    # Filter similarity matrix to include only ratings for the given user and similar movies\n",
    "    user_ratings = similarity_matrix.filter((col(\"userId\") == user_id) & (col(\"movieId_1\") == movie_id))\n",
    "    # Sort the ratings by similarity in descending order and select the top 2 most similar movies\n",
    "    user_ratings = user_ratings.sort(col(\"similarity\").desc()).limit(2)\n",
    "    # Calculate the predicted rating by computing a weighted average of the user's ratings for similar movies\n",
    "    user_ratings = user_ratings.withColumn(\"weighted_rating\", (col(\"rating\") * col(\"similarity\")).cast(FloatType()))\n",
    "    numerator = user_ratings.agg(sum(\"weighted_rating\")).collect()[0][0]\n",
    "    denominator = user_ratings.agg(sum(\"similarity\")).collect()[0][0]\n",
    "    \n",
    "    if denominator != 0:\n",
    "        predicted_rating = numerator / denominator\n",
    "    else:\n",
    "        predicted_rating = None\n",
    "    return predicted_rating\n",
    "\n",
    "\n",
    "df_user_movie_rating.show(10)\n",
    "\n",
    "predicted_rating1 = predict_user_rating(user_id=5, movie_id=1, similarity_matrix=df_user_movie_rating)\n",
    "predicted_rating2 = predict_user_rating(user_id=5, movie_id=3, similarity_matrix=df_user_movie_rating)\n",
    "print(\"predicted_rating1\", predicted_rating1)\n",
    "print(\"predicted_rating2\", predicted_rating2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
