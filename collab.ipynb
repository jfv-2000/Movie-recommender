{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 00:04:06 WARN SimpleFunctionRegistry: The function dot_udf replaced a previously registered function.\n",
      "+-------+---+---+---+---+---+---+\n",
      "|movieId|  1|  2|  3|  4|  5|  6|\n",
      "+-------+---+---+---+---+---+---+\n",
      "|      1|  1|  0|  2|  0|  0|  1|\n",
      "|      2|  0|  0|  4|  2|  0|  0|\n",
      "|      3|  3|  5|  0|  4|  4|  3|\n",
      "|      4|  0|  4|  1|  0|  3|  0|\n",
      "|      5|  0|  0|  2|  5|  4|  3|\n",
      "|      6|  5|  0|  0|  0|  2|  0|\n",
      "|      7|  0|  4|  3|  0|  0|  0|\n",
      "|      8|  0|  0|  0|  4|  0|  2|\n",
      "|      9|  5|  0|  4|  0|  0|  0|\n",
      "|     10|  0|  2|  3|  0|  0|  0|\n",
      "+-------+---+---+---+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/homebrew/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|movieId|movieId_1|          similarity|\n",
      "+-------+---------+--------------------+\n",
      "|      1|        2|-0.17854212213729673|\n",
      "|      1|        3| 0.41403933560541256|\n",
      "|      1|        4|-0.10245014273309601|\n",
      "|      1|        5|-0.30895719032666236|\n",
      "|      1|        6|  0.5870395085642741|\n",
      "|      2|        1|-0.17854212213729673|\n",
      "|      2|        3| -0.5262348115842176|\n",
      "|      2|        4| 0.46800784077976626|\n",
      "|      2|        5| 0.39891071573694176|\n",
      "|      2|        6| -0.3064397582621859|\n",
      "+-------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 562:===========================================>           (51 + 8) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|movieId|                   1|                   2|                  3|                   4|                   5|                   6|\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|      6|  0.5870395085642741| -0.3064397582621859| 0.5063696835418333| -0.2353393621658208|-0.21591675854376524|                 0.0|\n",
      "|      1|                 0.0|-0.17854212213729673|0.41403933560541256|-0.10245014273309601|-0.30895719032666236|  0.5870395085642741|\n",
      "|      3| 0.41403933560541256| -0.5262348115842176|                0.0| -0.6239806502223061| -0.2842676218074806|  0.5063696835418333|\n",
      "|      2|-0.17854212213729673|                 0.0|-0.5262348115842176| 0.46800784077976626| 0.39891071573694176| -0.3064397582621859|\n",
      "|      4|-0.10245014273309601| 0.46800784077976626|-0.6239806502223061|                 0.0| 0.45873490213598356| -0.2353393621658208|\n",
      "|      5|-0.30895719032666236| 0.39891071573694176|-0.2842676218074806| 0.45873490213598356|                 0.0|-0.21591675854376524|\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import DenseVector, Vectors\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf, col, when, count, sum as pyspark_sum\n",
    "\n",
    "#Initialize a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"recommenderTest\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"20000\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "# --- Data Pre-Processing ---\n",
    "def pearson_average(v):\n",
    "    \"\"\"\n",
    "    Computes the Pearson average of a PySpark Vector.\n",
    "    Returns a new Vector with the Pearson average.\n",
    "    \"\"\"\n",
    "    #divide the sum of the vector by the length of non-zero elements\n",
    "    sum_nonzero = sum(v)\n",
    "    count_nonzero = len([e for e in v if e != 0])\n",
    "    mean = sum_nonzero / count_nonzero\n",
    "    # now subtract the mean from each non zero element\n",
    "    v2 = [e - mean if e != 0 else 0 for e in v]\n",
    "    #convert to dense vector\n",
    "    return Vectors.dense(v2)\n",
    "\n",
    "def co_sym (x, y):\n",
    "    pearson1 = x\n",
    "    pearson2 = y\n",
    "    return float(pearson1.dot(pearson2)/(Vectors.norm(pearson1,2)*Vectors.norm(pearson2,2)))\n",
    "\n",
    "dot_udf = udf(co_sym, DoubleType())\n",
    "# dot_udf = udf(lambda x, y: float(x.dot(y)/(Vectors.norm(x,2)*Vectors.norm(y,2))), DoubleType())\n",
    "spark.udf.register(\"dot_udf\", dot_udf)\n",
    "# Load the data into a DataFrame (userId, movieId, rating, timestamp)\n",
    "# File format: userId, movieId, rating, timestamp\n",
    "df = spark.read.csv(\"data/ratings_tiny.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"timestamp\") # drop timestamp column\n",
    "# Group by movieId and pivot the userId column\n",
    "df = df.groupBy(\"movieId\").pivot(\"userId\").agg({\"rating\": \"first\"}).fillna(0)\n",
    "# sort rows by movieId\n",
    "df = df.sort(\"movieId\")\n",
    "# df.show()\n",
    "df.select(df.columns[:10]).show(10)\n",
    "# Transpose the DataFrame to switch rows and columns\n",
    "df = df.toPandas().set_index(\"movieId\").transpose().reset_index()\n",
    "# Convert pandas DataFrame back to Spark DataFrame\n",
    "df = spark.createDataFrame(df)\n",
    "cols = df.columns[1:]\n",
    "#print(cols)\n",
    "# Assemble the columns into a vector column\n",
    "assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select('index', 'features')\n",
    "# df_vector.show(truncate=False)\n",
    "# Compute the similarity matrix using the dot product of normalized vectors\n",
    "similarity_matrix = df_vector.alias(\"a\").crossJoin(df_vector.alias(\"b\")) \\\n",
    "    .where(\"a.index != b.index\") \\\n",
    "    .selectExpr(\"a.index as movieId\", \"b.index as movieId_1\",\n",
    "                \"dot_udf(a.features, b.features) as similarity\")\n",
    "#output the similarity matrix\n",
    "similarity_matrix.show(10)\n",
    "# pivot the similarity matrix\n",
    "similarity_matrix = similarity_matrix.groupBy(\"movieId\").pivot(\"movieId_1\").agg({\"similarity\": \"first\"}).fillna(0)\n",
    "similarity_matrix.select(similarity_matrix.columns[:10]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
