{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 02:18:40 WARN SimpleFunctionRegistry: The function dot_udf replaced a previously registered function.\n",
      "similarity_matrix\n",
      "+-------+---------+----------+\n",
      "|movieId|movieId_1|similarity|\n",
      "+-------+---------+----------+\n",
      "|      4|        3|-0.6239...|\n",
      "|      4|        6|-0.2353...|\n",
      "|      4|        5|0.45873...|\n",
      "|      4|        1|-0.1024...|\n",
      "|      4|        2|0.46800...|\n",
      "|      3|        4|-0.6239...|\n",
      "|      3|        6|0.50636...|\n",
      "|      3|        5|-0.2842...|\n",
      "|      3|        1|0.41403...|\n",
      "|      3|        2|-0.5262...|\n",
      "+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|movieId|                   1|                   2|                  3|                   4|                   5|                   6|\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|      3| 0.41403933560541256| -0.5262348115842176|                0.0| -0.6239806502223061| -0.2842676218074806|  0.5063696835418333|\n",
      "|      1|                 0.0|-0.17854212213729673|0.41403933560541256|-0.10245014273309601|-0.30895719032666236|  0.5870395085642741|\n",
      "|      6|  0.5870395085642741| -0.3064397582621859| 0.5063696835418333| -0.2353393621658208|-0.21591675854376524|                 0.0|\n",
      "|      5|-0.30895719032666236| 0.39891071573694176|-0.2842676218074806| 0.45873490213598356|                 0.0|-0.21591675854376524|\n",
      "|      2|-0.17854212213729673|                 0.0|-0.5262348115842176| 0.46800784077976626| 0.39891071573694176| -0.3064397582621859|\n",
      "|      4|-0.10245014273309601| 0.46800784077976626|-0.6239806502223061|                 0.0| 0.45873490213598356| -0.2353393621658208|\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import DenseVector, Vectors\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf, col, when, count, sum as pyspark_sum\n",
    "\n",
    "#Initialize a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"recommenderTest\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"20000\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "# --- Data Pre-Processing ---\n",
    "def pearson_average(v):\n",
    "    \"\"\"\n",
    "    Computes the Pearson average of a PySpark Vector.\n",
    "    Returns a new Vector with the Pearson average.\n",
    "    \"\"\"\n",
    "    #divide the sum of the vector by the length of non-zero elements\n",
    "    sum_nonzero = sum(v)\n",
    "    count_nonzero = len([e for e in v if e != 0])\n",
    "    mean = sum_nonzero / count_nonzero\n",
    "    # now subtract the mean from each non zero element\n",
    "    v2 = [e - mean if e != 0 else 0 for e in v]\n",
    "    #convert to dense vector\n",
    "    return Vectors.dense(v2)\n",
    "\n",
    "def co_sym (x, y):\n",
    "    pearson1 = pearson_average(x)\n",
    "    pearson2 = pearson_average(y)\n",
    "    return float(pearson1.dot(pearson2)/(Vectors.norm(pearson1,2)*Vectors.norm(pearson2,2)))\n",
    "\n",
    "dot_udf = udf(co_sym, DoubleType())\n",
    "# dot_udf = udf(lambda x, y: float(x.dot(y)/(Vectors.norm(x,2)*Vectors.norm(y,2))), DoubleType())\n",
    "spark.udf.register(\"dot_udf\", dot_udf)\n",
    "\n",
    "# Load the data into a DataFrame (userId, movieId, rating, timestamp)\n",
    "# File format: userId, movieId, rating, timestamp\n",
    "df = spark.read.csv(\"data/ratings_tiny.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"timestamp\") # drop timestamp column\n",
    "\n",
    "# Group by movieId and pivot the userId column\n",
    "df = df.groupBy(\"movieId\").pivot(\"userId\").agg({\"rating\": \"first\"}).fillna(0)\n",
    "\n",
    "\n",
    "# Assemble the columns into a vector column\n",
    "assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select('movieId', 'features')\n",
    "df_vector = df_vector.repartition(10)\n",
    "\n",
    "# Compute the similarity matrix using the dot product of normalized vectors\n",
    "similarity_matrix = df_vector.alias(\"a\").crossJoin(df_vector.alias(\"b\")) \\\n",
    "    .where(\"a.movieId != b.movieId\") \\\n",
    "    .selectExpr(\"a.movieId as movieId\", \"b.movieId as movieId_1\",\n",
    "                \"dot_udf(a.features, b.features) as similarity\")\n",
    "\n",
    "#just show first 10 rows and 10 columns\n",
    "print(\"similarity_matrix\")\n",
    "similarity_matrix.show(10, 10)\n",
    "\n",
    "\n",
    "# pivot the similarity matrix\n",
    "similarity_matrix = similarity_matrix.groupBy(\"movieId\").pivot(\"movieId_1\").agg({\"similarity\": \"first\"}).fillna(0)\n",
    "similarity_matrix.select(similarity_matrix.columns[:10]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
