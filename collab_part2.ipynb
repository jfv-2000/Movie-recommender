{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Recommendations Engine (Part 2/3)\n",
    "\n",
    "> This notebook is part of a series of notebooks that will walk you through the process of building a good collaborative recommendations engine (while also including our mistakes that we did). The series is broken up into three parts. If you haven't already, we would recommend you to read the first part of the series before continuing on with this one (as we won't repeat the same explanations).\n",
    "\n",
    "- Part 1: Our Attempt at Building an Item-Item Collaborative Recommendations Engine\n",
    "- **Part 2: Fixing our Item-Item Collaborative Recommendations Engine**\n",
    "- Part 3: Improving our Collaborative Recommendations Engine by leveraging other techniques than Item-Item Collaborative Filtering...\n",
    "\n",
    "## Part 2: Fixing our Item-Item Collaborative Recommendations Engine\n",
    "In Part 1, we set up the ground foundation of our system and had some difficulties related to the way we can predict values efficiently for large datasets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, udf, row_number, expr, coalesce, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import  Vectors\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Spark Configuration/Setup\n",
    "Idem to Part 1, we will be using Spark.\n",
    "\n",
    "Throughout this program, we will also need to use udf-functions to create our own functions. They are implemented and registered in the following code-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 14:33:55 WARN Utils: Your hostname, Martin-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.53 instead (on interface en0)\n",
      "23/03/25 14:33:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 14:33:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.co_sym(x, y)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Initialization/Setup\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"recommenderTest\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"20000\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "\n",
    "# -------------- UDFs Helper's Functions Implementation --------------\n",
    "\n",
    "# Cosine Similarity Measure Calculation\n",
    "# Note: We won't be using Pearson anymore. More details will follow on why.\n",
    "def co_sym (x, y):\n",
    "    x1 = x\n",
    "    x2 = y\n",
    "    return float(x1.dot(x2)/(Vectors.norm(x1,2)*Vectors.norm(x2,2)))\n",
    "\n",
    "# -------------- UDFs Initialization --------------\n",
    "dot_udf = udf(co_sym, DoubleType())\n",
    "spark.udf.register(\"dot_udf\", dot_udf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation/Loading\n",
    "Data Preparation will be identical to Part 1. No preparation required except removing timestamp.\n",
    "\n",
    "For this notebook, we will be using ratings_small.csv to validate our method. Ratings_small.csv is a dataset part of MoviesLens (the file format is identical to the previous part). It contains 100,004 ratings and 3,671 tag applications across 9,742 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/ratings_small.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"timestamp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Modeling\n",
    "This step is crucial in order to build our recommender. This is where we will be building our item-item collaborative filtering model. We will be using the following steps:\n",
    "1. Create Utility Matrix (Item-Item) where the rows are the movies and the columns are the users (in our code: this is represented as df_user_movie_rating)\n",
    "2. Calculate the similarity between each pair of movies (in our code: this is represented as similarity_matrix) using the following formula (Cosine Similarity Measure):\n",
    "$$similarity = \\frac{\\sum_{u \\in U} (r_{u,i})(r_{u,j})}{\\sqrt{\\sum_{u \\in U} (r_{u,i})^2} \\sqrt{\\sum_{u \\in U} (r_{u,j})^2}}$$\n",
    "3. Build a new matrix which will contain all the movies combinations (similarity) for each user. This matrix will be used to predict the ratings for each user\n",
    "\n",
    "**Note**: \n",
    "In this notebook, we have some differences compared to Part 1 in our Model Implementation:\n",
    "- We are splitting our dataset into training (80%) and testing (20%) set. We were very careful to ensure that the testing set contains movies that are not in the training set (including any similarities values). This is to ensure that we are not overfitting our model. \n",
    "- As previously hinted, we will be using the Cosine Similarity Measure to calculate the similarity between each pair of movies. This is because the Pearson Correlation Measure is not suitable for our ways of predicting values. By using Pearson, we were predicting values that weren't always included in 0-5 which is not desirable... The Cosine Similarity Measure is more suitable for sparse datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|movieId|movieId_1|similarity|\n",
      "+-------+---------+----------+\n",
      "|   1393|    72356|       0.0|\n",
      "|   1393|     1875|0.13639...|\n",
      "|   1393|    95508|0.11356...|\n",
      "|   1393|     1327|0.13387...|\n",
      "|   1393|    59022|0.16576...|\n",
      "|   1393|      569|0.10198...|\n",
      "|   1393|    26680|0.05962...|\n",
      "|   1393|     2796|0.07571...|\n",
      "|   1393|     8923|0.06602...|\n",
      "|   1393|     6754|0.12117...|\n",
      "+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+-------------------+\n",
      "|userId|movieId|rating|movieId_1|         similarity|\n",
      "+------+-------+------+---------+-------------------+\n",
      "|     1|   1343|   2.0|     1393|0.25837203347039406|\n",
      "|     1|   1343|   2.0|    72356|                0.0|\n",
      "|     1|   1343|   2.0|     1875|0.04559607525875532|\n",
      "|     1|   1343|   2.0|    95508|0.23728949893812476|\n",
      "|     1|   1343|   2.0|     1327| 0.2191986497404764|\n",
      "|     1|   1343|   2.0|    59022|0.14513629044340784|\n",
      "|     1|   1343|   2.0|      569|0.04447007244699521|\n",
      "|     1|   1343|   2.0|    26680|0.11389895949029989|\n",
      "|     1|   1343|   2.0|     2796|0.19774124911510396|\n",
      "|     1|   1343|   2.0|     8923| 0.1261172499428486|\n",
      "+------+-------+------+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# -------------- Data Initialization --------------\n",
    "# Data Splitting (80% training, 20% testing)\n",
    "training, test = df.randomSplit([0.8, 0.2])\n",
    "# DF to be used for Training (easier to use another variable name for testing purposes)\n",
    "df = training\n",
    "# DF to be used for User-Movie-Rating Matrix (will be used later on - Setting initial state for now...)\n",
    "df_user_movie_rating = df\n",
    "\n",
    "# -------------- Building Similarity Matrix --------------\n",
    "df = df.groupBy(\"movieId\").pivot(\"userId\").agg({\"rating\": \"first\"}).fillna(0)\n",
    "df = df.sort(\"movieId\")\n",
    "\n",
    "# Build Vector Columns from DF Matrix\n",
    "assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select('movieId', 'features')\n",
    "df_vector = df_vector.repartition(10)\n",
    "\n",
    "# Compute Cosine Similarity Measure to fill data into Similarity Matrix\n",
    "similarity_matrix = df_vector.alias(\"a\").crossJoin(df_vector.alias(\"b\")) \\\n",
    "    .where(\"a.movieId != b.movieId\") \\\n",
    "    .selectExpr(\"a.movieId as movieId\", \"b.movieId as movieId_1\",\n",
    "                \"dot_udf(a.features, b.features) as similarity\")\n",
    "similarity_matrix.show(10, 10)\n",
    "\n",
    "# Build User-Movie-Rating Matrix (where for each user, we have all the movies combinations with the similarity values)\n",
    "df_user_movie_rating = df_user_movie_rating.join(similarity_matrix, df_user_movie_rating.movieId == similarity_matrix.movieId, how='left').drop(similarity_matrix.movieId)\n",
    "df_user_movie_rating = df_user_movie_rating.withColumnRenamed(\"similarity.movieId\", \"movie2\")\n",
    "df_user_movie_rating.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Prediction\n",
    "Now that we have a dataframe containing all the possible movies similarity combinations for each users, we can use this dataframe to predict (*for real, this time...*) the ratings for each user. We need to be able to predict ratings in order to be able to evaluate our model. Here are the steps we will be using:\n",
    "1. Join the Test Set with the User-Movie Rating Matrix (we will then have all the possible movies similarities combinations for each user in the test set)\n",
    "2. Get top-N similar movies (Default=2) for each user (where N is a parameter that we can tune - more details on this later)\n",
    "3. Calculate the predicted rating (in a Dataframe) for each user using the following formula:\n",
    "$$predictedRating = \\frac{\\sum_{i=1}^{N} (similarity_{i,j})(rating_{i,j})}{\\sum_{i=1}^{N} (similarity_{i,j})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 101:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------------------+\n",
      "|userId|movieId|rating|  predicted_rating|\n",
      "+------+-------+------+------------------+\n",
      "|     2|    589|   5.0| 3.616155322230661|\n",
      "|     3|   1378|   4.0|3.4536925810355763|\n",
      "|     4|   1194|   5.0| 4.168033799934364|\n",
      "|     4|   1219|   5.0| 4.821456884742036|\n",
      "|     4|   1344|   5.0|  4.81944174326345|\n",
      "|     4|   1377|   3.0| 4.221582692338619|\n",
      "|     4|   2902|   2.0| 3.745014821592258|\n",
      "|     5|   4963|   3.0| 3.705112158996253|\n",
      "|     6|   1747|   2.0| 2.814214525105227|\n",
      "|     6|   2502|   3.5|3.0026333945756085|\n",
      "+------+-------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# N-Value (Top N Similar Movies to be used for Prediction) - We will use 4 for now\n",
    "nValue = 16\n",
    "\n",
    "# Create DF Alias To Use In Prediction Computations\n",
    "test_df_alias = test.alias(\"tst\")\n",
    "sm_alias = df_user_movie_rating.alias(\"sm\")\n",
    "\n",
    "# Window Specification to help with Ranking Top N Similar Movies\n",
    "window_spec = Window.partitionBy(\"tst.userId\", \"tst.movieId\").orderBy(col(\"sm.similarity\").desc())\n",
    "\n",
    "# Join Similarity Matrix with Test DF to get the top N similar movies for each user-movie pair in the test set\n",
    "joined_df = test_df_alias.join(sm_alias, (col(\"tst.userId\") == col(\"sm.userId\")) & (col(\"tst.movieId\") == col(\"sm.movieId_1\"))).withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") <= nValue)\n",
    "\n",
    "# Compute Weighted Rating\n",
    "weighted_df = joined_df.withColumn(\"weighted_rating\", col(\"sm.rating\") * col(\"sm.similarity\"))\n",
    "\n",
    "# Compute Predicted Rating (and save it in Result DF - where we are storing: userId, movieId, rating, predicted_rating)\n",
    "result_df = (weighted_df\n",
    "            .groupBy(\"tst.userId\", \"tst.movieId\", \"tst.rating\")\n",
    "            .agg(spark_sum(\"weighted_rating\").alias(\"sum_weighted_rating\"), spark_sum(\"sm.similarity\").alias(\"sum_similarity\"))\n",
    "            .withColumn(\"predicted_rating\", expr(\"sum_weighted_rating / sum_similarity\"))\n",
    "            .drop(\"sum_weighted_rating\", \"sum_similarity\")).withColumn(\"predicted_rating\", coalesce(col(\"predicted_rating\"), lit(0.0)))\n",
    "result_df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Model Evaluation\n",
    "Great! We have a model that can predict ratings for each user. Now, we need to evaluate our model. We will be using the Root Mean Squared Error (RMSE) to evaluate our model (by trying to predict values in our test state). RMSE is a measure of how close a fitted line is to data points. The lower the RMSE, the better our model is. The formula is as follows:\n",
    "$$RMSE = \\sqrt{\\ \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2}$$\n",
    "\n",
    "**Note**: As previously mentionned, we made sure to not contamined our test set by movies that are in the training set. This is to ensure that we are not overfitting our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.8938\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"predicted_rating\")\n",
    "rmse = evaluator.evaluate(result_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.4f}\".format(rmse))\n",
    "\n",
    "spark.stop() # Stop Spark Session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Performance\n",
    "Now that we have ran our model, we can evaluate its performance (by looking at RMSE, Time to Run Model) while tuning its parameters (in our case: value of Top-N values used in similarity calculations). In order to have a better overview, we ran our model 4 times with different values of Top-N (2, 4, 6, 8). Here are the results: \n",
    "<table>\n",
    "<tr>\n",
    "<th>Test 1</th>\n",
    "<th>Test 2</th>\n",
    "<th>Test 3</th>\n",
    "<th>Test 4</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 2 | 1.0185 | 42.523min |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 4 | 0.9362 | 41.975min |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 6 | 0.8972 | 42.73min |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 16 | 0.8938 | 43.02min |\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr> </table>\n",
    "\n",
    "By looking at the result, we can see that a value of Top-N=6 is the best value for our model. This is because it has a low RMSE value while being faster than top-16. If you compare it with the Performance Scale shown in the slides, our model performs exactly as expected for a basic collaborative filtering technique. \n",
    "\n",
    "For now, we won't use other Evaluation Techniques, because we aren't satisfied with the rapidity of our model. We will be looking into improving our model in Part 3, and from there will be able to evaluate this model using other techniques than just RMSE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Now What?\n",
    "Although we have a model that can predict ratings for each user, we still have some issues with our model. \n",
    "\n",
    "> TL;DR Parallelization itself doesn't guarantee optimal performance. You must also choose the right algorithm for the job.\n",
    "\n",
    "1. **Model is slow**: The main issues are that implementing item-item this way without using factorization/KNN techniques isn't very efficient due to the necessity to manage a lot of large dataframes conversions. We are using a lot of join operations (to build similarity matrix, to predict and to evaluate) which are very costly on a single machine. Our code would be much faster if we were using a distributed system (with multiple nodes using `Spark broadcasting`), thanks to our Spark Code! The issue is not because we aren't using Spark parallelizations, but because we are using inefficient methods to build our model.\n",
    "2. **Model is not very accurate**: We are using a very simple model (item-item collaborative filtering) and we are not using any other techniques to improve our model (adding biases, normalizations, etc would simply make our systems even more slow). We will be using other techniques in the next part of the series to see if we can improve our model.\n",
    "\n",
    "We will be fixing these issues in the next part of the series.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
