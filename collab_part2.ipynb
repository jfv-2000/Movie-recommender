{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Recommendations Engine (Part 2/3)\n",
    "\n",
    "> This notebook is part of a series of notebooks that will walk you through the process of building a good collaborative recommendations engine (while also including our mistakes that we did). The series is broken up into three parts. If you haven't already, we would recommend you to read the first part of the series before continuing on with this one (as we won't repeat the same explanations).\n",
    "\n",
    "- Part 1: Our Attempt at Building an Item-Item Collaborative Recommendations Engine\n",
    "- **Part 2: Fixing our Item-Item Collaborative Recommendations Engine**\n",
    "- Part 3: Improving our Collaborative Recommendations Engine by leveraging other techniques than Item-Item Collaborative Filtering...\n",
    "\n",
    "## Part 2: Fixing our Item-Item Collaborative Recommendations Engine\n",
    "In Part 1, we set up the ground foundation of our system and had some difficulties related to the way we can predict values efficiently for large datasets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, udf, row_number, expr, coalesce, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import  Vectors\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Spark Configuration/Setup\n",
    "Idem to Part 1, we will be using Spark.\n",
    "\n",
    "Throughout this program, we will also need to use udf-functions to create our own functions. They are implemented and registered in the following code-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/25 01:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.co_sym(x, y)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Initialization/Setup\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"recommenderTest\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"20000\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "\n",
    "# -------------- UDFs Helper's Functions Implementation --------------\n",
    "\n",
    "# Cosine Similarity Measure Calculation\n",
    "# Note: We won't be using Pearson anymore. More details will follow on why.\n",
    "def co_sym (x, y):\n",
    "    x1 = x\n",
    "    x2 = y\n",
    "    return float(x1.dot(x2)/(Vectors.norm(x1,2)*Vectors.norm(x2,2)))\n",
    "\n",
    "# -------------- UDFs Initialization --------------\n",
    "dot_udf = udf(co_sym, DoubleType())\n",
    "spark.udf.register(\"dot_udf\", dot_udf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation/Loading\n",
    "Data Preparation will be identical to Part 1. No preparation required except removing timestamp.\n",
    "\n",
    "For this notebook, we will be using ratings_small.csv to validate our method. Ratings_small.csv is a dataset part of MoviesLens (the file format is identical to the previous part). It contains 100,004 ratings and 3,671 tag applications across 9,742 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/ratings_tiny.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"timestamp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Modeling\n",
    "This step is crucial in order to build our recommender. This is where we will be building our item-item collaborative filtering model. We will be using the following steps:\n",
    "1. Create Utility Matrix (Item-Item) where the rows are the movies and the columns are the users (in our code: this is represented as df_user_movie_rating)\n",
    "2. Calculate the similarity between each pair of movies (in our code: this is represented as similarity_matrix) using the following formula (Cosine Similarity Measure):\n",
    "$$similarity = \\frac{\\sum_{u \\in U} (r_{u,i})(r_{u,j})}{\\sqrt{\\sum_{u \\in U} (r_{u,i})^2} \\sqrt{\\sum_{u \\in U} (r_{u,j})^2}}$$\n",
    "3. Build a new matrix which will contain all the movies combinations (similarity) for each user. This matrix will be used to predict the ratings for each user\n",
    "\n",
    "**Note**: \n",
    "In this notebook, we have some differences compared to Part 1 in our Model Implementation:\n",
    "- We are splitting our dataset into training (80%) and testing (20%) set. We were very careful to ensure that the testing set contains movies that are not in the training set (including any similarities values). This is to ensure that we are not overfitting our model. \n",
    "- As previously hinted, we will be using the Cosine Similarity Measure to calculate the similarity between each pair of movies. This is because the Pearson Correlation Measure is not suitable for our ways of predicting values. By using Pearson, we were predicting values that weren't always included in 0-5 which is not desirable... The Cosine Similarity Measure is more suitable for sparse datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|movieId|movieId_1|similarity|\n",
      "+-------+---------+----------+\n",
      "|      6|        2|0.46774...|\n",
      "|      6|        3|0.05216...|\n",
      "|      6|        5|0.52164...|\n",
      "|      6|        1|0.37918...|\n",
      "|      6|        4|0.63407...|\n",
      "|      2|        6|0.46774...|\n",
      "|      2|        3|       0.0|\n",
      "|      2|        5|0.65493...|\n",
      "|      2|        1|0.06589...|\n",
      "|      2|        4|0.36794...|\n",
      "+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+-------------------+\n",
      "|movieId|userId|rating|movieId_1|         similarity|\n",
      "+-------+------+------+---------+-------------------+\n",
      "|      1|     1|     1|        4|0.12122603684516312|\n",
      "|      1|     1|     1|        5| 0.3141499997733563|\n",
      "|      1|     1|     1|        3| 0.3839611108341021|\n",
      "|      1|     1|     1|        2|0.06589329806578535|\n",
      "|      1|     1|     1|        6|0.37918477623364993|\n",
      "|      1|     6|     5|        4|0.12122603684516312|\n",
      "|      1|     6|     5|        5| 0.3141499997733563|\n",
      "|      1|     6|     5|        3| 0.3839611108341021|\n",
      "|      1|     6|     5|        2|0.06589329806578535|\n",
      "|      1|     6|     5|        6|0.37918477623364993|\n",
      "+-------+------+------+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------- Data Initialization --------------\n",
    "# Data Splitting (80% training, 20% testing)\n",
    "training, test = df.randomSplit([0.8, 0.2])\n",
    "# DF to be used for Training (easier to use another variable name for testing purposes)\n",
    "df = training\n",
    "# DF to be used for User-Movie-Rating Matrix (will be used later on - Setting initial state for now...)\n",
    "df_user_movie_rating = df\n",
    "\n",
    "# -------------- Building Similarity Matrix --------------\n",
    "df = df.groupBy(\"movieId\").pivot(\"userId\").agg({\"rating\": \"first\"}).fillna(0)\n",
    "df = df.sort(\"movieId\")\n",
    "\n",
    "# Build Vector Columns from DF Matrix\n",
    "assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select('movieId', 'features')\n",
    "df_vector = df_vector.repartition(10)\n",
    "\n",
    "# Compute Cosine Similarity Measure to fill data into Similarity Matrix\n",
    "similarity_matrix = df_vector.alias(\"a\").crossJoin(df_vector.alias(\"b\")) \\\n",
    "    .where(\"a.movieId != b.movieId\") \\\n",
    "    .selectExpr(\"a.movieId as movieId\", \"b.movieId as movieId_1\",\n",
    "                \"dot_udf(a.features, b.features) as similarity\")\n",
    "similarity_matrix.show(10, 10)\n",
    "\n",
    "# Build User-Movie-Rating Matrix (where for each user, we have all the movies combinations with the similarity values)\n",
    "df_user_movie_rating = df_user_movie_rating.join(similarity_matrix, df_user_movie_rating.movieId == similarity_matrix.movieId, how='left').drop(similarity_matrix.movieId)\n",
    "df_user_movie_rating = df_user_movie_rating.withColumnRenamed(\"similarity.movieId\", \"movie2\")\n",
    "df_user_movie_rating.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Prediction\n",
    "Now that we have a dataframe containing all the possible movies similarity combinations for each users, we can use this dataframe to predict (*for real, this time...*) the ratings for each user. We need to be able to predict ratings in order to be able to evaluate our model. Here are the steps we will be using:\n",
    "1. Join the Test Set with the User-Movie Rating Matrix (we will then have all the possible movies similarities combinations for each user in the test set)\n",
    "2. Get top-N similar movies (Default=2) for each user (where N is a parameter that we can tune - more details on this later)\n",
    "3. Calculate the predicted rating (in a Dataframe) for each user using the following formula:\n",
    "$$predictedRating = \\frac{\\sum_{i=1}^{N} (similarity_{i,j})(rating_{i,j})}{\\sum_{i=1}^{N} (similarity_{i,j})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------------------+\n",
      "|userId|movieId|rating|  predicted_rating|\n",
      "+------+-------+------+------------------+\n",
      "|     3|      1|     3|3.4531000184104164|\n",
      "|     4|      3|     1|               3.0|\n",
      "|     5|      6|     3|4.5486428500861535|\n",
      "|     7|      2|     4|               0.0|\n",
      "|    10|      3|     3|               0.0|\n",
      "|    11|      3|     5|3.0924938827309423|\n",
      "|    12|      5|     5|               3.0|\n",
      "+------+-------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# N-Value (Top N Similar Movies to be used for Prediction) - We will use 2 for now\n",
    "nValue = 2\n",
    "\n",
    "# Create DF Alias To Use In Prediction Computations\n",
    "test_df_alias = test.alias(\"tst\")\n",
    "sm_alias = df_user_movie_rating.alias(\"sm\")\n",
    "\n",
    "# Window Specification to help with Ranking Top N Similar Movies\n",
    "window_spec = Window.partitionBy(\"tst.userId\", \"tst.movieId\").orderBy(col(\"sm.similarity\").desc())\n",
    "\n",
    "# Join Similarity Matrix with Test DF to get the top N similar movies for each user-movie pair in the test set\n",
    "joined_df = test_df_alias.join(sm_alias, (col(\"tst.userId\") == col(\"sm.userId\")) & (col(\"tst.movieId\") == col(\"sm.movieId_1\"))).withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") <= nValue)\n",
    "\n",
    "# Compute Weighted Rating\n",
    "weighted_df = joined_df.withColumn(\"weighted_rating\", col(\"sm.rating\") * col(\"sm.similarity\"))\n",
    "\n",
    "# Compute Predicted Rating (and save it in Result DF - where we are storing: userId, movieId, rating, predicted_rating)\n",
    "result_df = (weighted_df\n",
    "            .groupBy(\"tst.userId\", \"tst.movieId\", \"tst.rating\")\n",
    "            .agg(spark_sum(\"weighted_rating\").alias(\"sum_weighted_rating\"), spark_sum(\"sm.similarity\").alias(\"sum_similarity\"))\n",
    "            .withColumn(\"predicted_rating\", expr(\"sum_weighted_rating / sum_similarity\"))\n",
    "            .drop(\"sum_weighted_rating\", \"sum_similarity\")).withColumn(\"predicted_rating\", coalesce(col(\"predicted_rating\"), lit(0.0)))\n",
    "result_df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Model Evaluation\n",
    "Great! We have a model that can predict ratings for each user. Now, we need to evaluate our model. We will be using the Root Mean Squared Error (RMSE) to evaluate our model (by trying to predict values in our test state). RMSE is a measure of how close a fitted line is to data points. The lower the RMSE, the better our model is. The formula is as follows:\n",
    "$$RMSE = \\sqrt{\\ \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2}$$\n",
    "\n",
    "**Note**: As previously mentionned, we made sure to not contamined our test set by movies that are in the training set. This is to ensure that we are not overfitting our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.3677\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"predicted_rating\")\n",
    "rmse = evaluator.evaluate(result_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.4f}\".format(rmse))\n",
    "\n",
    "spark.stop() # Stop Spark Session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Performance\n",
    "\n",
    "TODO: Add details on values of RMSE based on different values of top-n + ensure we have the same seed to have the same ground truth\n",
    "and Explain where is RMSE and if it's good and if it makes sense compare to netflix prize scale...\n",
    "<table>\n",
    "<tr>\n",
    "<th>Test 1</th>\n",
    "<th>Test 2</th>\n",
    "<th>Test 3</th>\n",
    "<th>Test 4</th>\n",
    "</tr>\n",
    "<tr>\n",
    "\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "<td>\n",
    "\n",
    "| Top-N | RMSE Value | Time |\n",
    "|--|--|--|\n",
    "| 1 | 2 | 3 |\n",
    "\n",
    "</td>\n",
    "\n",
    "</tr> </table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Now What?\n",
    "Although we have a model that can predict ratings for each user, we still have some issues with our model. \n",
    "1. **Model is slow**: The main issues are that implementing item-item this way without using factorization/KNN techniques isn't very efficient due to the necessity to manage a lot of large dataframes conversions. We are using a lot of join operations (to build similarity matrix, to predict and to evaluate) which are very costly on a single machine. Our code would be much faster if we were using a distributed system (with multiple nodes), thanks to our Spark Code! The issue is not because we aren't using Spark parallelizations, but because we are using inefficient methods to build our model.\n",
    "2. **Model is not very accurate**: We are using a very simple model (item-item collaborative filtering) and we are not using any other techniques to improve our model (adding biases, normalizations, etc would simply make our systems even more slow). We will be using other techniques in the next part of the series to improve our model.\n",
    "\n",
    "We will be fixing these issues in the next part of the series.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
