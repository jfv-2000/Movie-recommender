{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Recommendations Engine (Part 1/3)\n",
    "\n",
    "> This notebook is part of a series of notebooks that will walk you through the process of building a good collaborative recommendations engine (while also including our mistakes that we did). The series is broken up into three parts:\n",
    "\n",
    "- **Part 1: Our Attempt at Building an Item-Item Collaborative Recommendations Engine**\n",
    "- Part 2: Fixing our Item-Item Collaborative Recommendations Engine\n",
    "- Part 3: Improving our Collaborative Recommendations Engine by leveraging other techniques than Item-Item Collaborative Filtering...\n",
    "\n",
    "## Part 1: Our Attempt at Building an Item-Item Collaborative Recommendations Engine\n",
    "When we started on our recommender system, we would have never expected that we would have to go through so many iterations to get to a working model. We started off with a simple item-item collaborative filtering model (based on the slides), but we quickly realized that it's harder than it look like to build a model that is scalable...\n",
    "\n",
    "Throughout the process, we will be using Movies Dataset which consists of X ratings from Y users on Z movies (where the row format is: `movieId,userId,rating,timestamp`). The endgoal is to be able to use our Kaggle Dataset of 100k ratings, but to start off, we will be using a smaller homemade dataset based on the slides to validate our methods.\n",
    "\n",
    "Lastly, it's important to mentionned that we will be using a sparse utility matrix to store our ratings. We won't have to collect the data in the utility matrix, since it's already provided by the dataset. Ratings vary from 0 to 5, and we will be using a 0 to represent a missing rating. Our endgoal is to be able to extrapolate unknown ratings from the known ones, while also evaluate these extrapolate methods to measure the success/performance of our system.\n",
    "\n",
    "More details about our techniques will be provided throughout the notebook, but for now let's start!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, sum as spark_sum\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import  Vectors\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Spark Configuration/Setup\n",
    "We will be using Spark to build our recommender system. Spark is a distributed computing framework that allows us to run our code on a cluster of machines. It's a great tool to use when we want to scale our code to a large dataset. We will be using the `pyspark` library to run our code on a local machine. We will be using the `pyspark.sql` library to create our dataframes, and the `pyspark.ml` library to build our recommender system.\n",
    "\n",
    "Throughout the program, we will also need to use udf-functions to create our own functions. They are implemented and registered in the following code-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.dot_product_divided_by_sum(v1, v2)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Initialization/Setup\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"recommenderTest\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"20000\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "\n",
    "\n",
    "# -------------- UDFs Helper's Functions Implementation --------------\n",
    "\n",
    "# Pearson Correlation Average Calculation (for Rx' and Ry')\n",
    "# Note: Our average function is not scalable, but it is good enough for our purposes for now, because\n",
    "# we won't move forward with it when proceeding with larger datasets (more details soon...)\n",
    "def pearson_average(v):\n",
    "    sum_nonzero = sum(v)\n",
    "    count_nonzero = len([e for e in v if e != 0])\n",
    "    mean = sum_nonzero / count_nonzero\n",
    "    v2 = [e - mean if e != 0 else 0 for e in v] # now subtract the mean from each non zero element\n",
    "    return Vectors.dense(v2)\n",
    "\n",
    "# Pearson Correlation Coefficient Calculation\n",
    "# Note: Although the correlation average calculation isn't scalable, the dot product/norm calculation is.\n",
    "def co_sym (x, y):\n",
    "    pearson1 = pearson_average(x)\n",
    "    pearson2 = pearson_average(y)\n",
    "    return float(pearson1.dot(pearson2)/(Vectors.norm(pearson1,2)*Vectors.norm(pearson2,2)))\n",
    "\n",
    "# Matrix Dot Product Calculation: (v1 * v2) / sum(v1)\n",
    "def dot_product_divided_by_sum(v1, v2):\n",
    "    dot_product = float(v1.dot(v2))\n",
    "    sum_v1 = float(v1.sum())\n",
    "    if sum_v1 == 0:\n",
    "        return 0.0\n",
    "    return dot_product/sum_v1\n",
    "\n",
    "# -------------- UDFs Initialization --------------\n",
    "dot_udf = udf(co_sym, DoubleType())\n",
    "spark.udf.register(\"dot_udf\", dot_udf)\n",
    "spark.udf.register(\"dot_product_divided_by_sum\", dot_product_divided_by_sum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation/Loading\n",
    "Unlike content-based, we don't need to do any data preparation for collaborative filtering (except removing timestamp since it will not be used). We will be directly using the `ratings.csv` file (format: userId, movieId, rating, timestamp).\n",
    "\n",
    "For this notebook, we will be using ratings_tiny.csv to validate our method. Ratings_tiny.csv is the same as the example in the slides (p.27 in Collaborative-Slideset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/ratings_tiny.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"timestamp\")\n",
    "\n",
    "\n",
    "# -------------- Data Initialization --------------\n",
    "# DF to be used for User-Movie-Rating Matrix (will be used later on - Setting initial state for now...)\n",
    "df_user_movie_rating = df\n",
    "# DF to be used for Tentative 2... (will be used later on - Setting initial state for now...)\n",
    "df2 = df_user_movie_rating.groupBy(\"userId\").pivot(\"movieId\").agg({\"rating\": \"first\"}).fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Data Modeling\n",
    "This step is crucial in order to build our recommender. This is where we will be building our item-item collaborative filtering model. We will be using the following steps:\n",
    "1. Create Utility Matrix (Item-Item) where the rows are the movies and the columns are the users (in our code: this is represented as df_user_movie_rating)\n",
    "2. Calculate the similarity between each pair of movies (in our code: this is represented as similarity_matrix) using the following formula (Pearson Correlation Coefficient):\n",
    "$$similarity = \\frac{\\sum_{u \\in U} (r_{u,i} - \\bar{r_i})(r_{u,j} - \\bar{r_j})}{\\sqrt{\\sum_{u \\in U} (r_{u,i} - \\bar{r_i})^2} \\sqrt{\\sum_{u \\in U} (r_{u,j} - \\bar{r_j})^2}}$$\n",
    "3. Build a new matrix which will contain all the movies combinations (similarity) for each user. This matrix will be used to predict the ratings for each user\n",
    "\n",
    "**Note**: In this notebook, we won't split our dataset into training and testing set. We will be using the whole dataset to build our model in order to validate our model with the example in the slides. In the next notebook, we will be splitting our dataset into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|movieId|movieId_1|similarity|\n",
      "+-------+---------+----------+\n",
      "|      4|        3|-0.6239...|\n",
      "|      4|        6|-0.2353...|\n",
      "|      4|        5|0.45873...|\n",
      "|      4|        1|-0.1024...|\n",
      "|      4|        2|0.46800...|\n",
      "|      3|        4|-0.6239...|\n",
      "|      3|        6|0.50636...|\n",
      "|      3|        5|-0.2842...|\n",
      "|      3|        1|0.41403...|\n",
      "|      3|        2|-0.5262...|\n",
      "+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+------+------+---------+----------+\n",
      "|movieId|userId|rating|movieId_1|similarity|\n",
      "+-------+------+------+---------+----------+\n",
      "|      1|     1|     1|        2|-0.1785...|\n",
      "|      1|     1|     1|        5|-0.3089...|\n",
      "|      1|     1|     1|        6|0.58703...|\n",
      "|      1|     1|     1|        3|0.41403...|\n",
      "|      1|     1|     1|        4|-0.1024...|\n",
      "|      1|     3|     3|        2|-0.1785...|\n",
      "|      1|     3|     3|        5|-0.3089...|\n",
      "|      1|     3|     3|        6|0.58703...|\n",
      "|      1|     3|     3|        3|0.41403...|\n",
      "|      1|     3|     3|        4|-0.1024...|\n",
      "+-------+------+------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Building Similarity Matrix --\n",
    "df = df.groupBy(\"movieId\").pivot(\"userId\").agg({\"rating\": \"first\"}).fillna(0)\n",
    "\n",
    "# Build Vector Columns from DF Matrix\n",
    "assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select('movieId', 'features')\n",
    "df_vector = df_vector.repartition(10)\n",
    "\n",
    "# Compute Pearson Correlation to fill data into Similarity Matrix\n",
    "similarity_matrix = df_vector.alias(\"a\").crossJoin(df_vector.alias(\"b\")) \\\n",
    "    .where(\"a.movieId != b.movieId\") \\\n",
    "    .selectExpr(\"a.movieId as movieId\", \"b.movieId as movieId_1\",\n",
    "                \"dot_udf(a.features, b.features) as similarity\")\n",
    "similarity_matrix.show(10, 10)\n",
    "\n",
    "# Build User-Movie-Rating Matrix (where for each user, we have all the movies combinations with the similarity values)\n",
    "df_user_movie_rating = df_user_movie_rating.join(similarity_matrix, df_user_movie_rating.movieId == similarity_matrix.movieId, how='left').drop(similarity_matrix.movieId)\n",
    "df_user_movie_rating = df_user_movie_rating.withColumnRenamed(\"similarity.movieId\", \"movie2\")\n",
    "df_user_movie_rating.show(10,10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data Prediction\n",
    "Now that we have a dataframe containing all the possible movies similarity combinations for each users, we can use this dataframe **to attempt** to predict the ratings for each user. We need to be able to predict ratings in order to be able to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Rating for User Id = 5 with Movie Id = 1 is:  2.5864068884261053\n"
     ]
    }
   ],
   "source": [
    "# -- Predicting User Rating Function where you need to pass a userId, movieId + dataframe --\n",
    "def predict_user_rating(user_id, movie_id, smDF): # Can't register a UDF with a DF as input\n",
    "    \n",
    "    # Filter similarity matrix to include only ratings for the given user and similar movies\n",
    "    user_ratings = smDF.filter((col(\"userId\") == user_id) & (col(\"movieId_1\") == movie_id))\n",
    "    \n",
    "    # Sort the ratings by similarity in descending order and select the top 2 most similar movies\n",
    "    user_ratings = user_ratings.sort(col(\"similarity\").desc()).limit(2)\n",
    "    \n",
    "    # Calculate the predicted rating by computing a weighted average of the user's ratings for similar movies\n",
    "    user_ratings = user_ratings.withColumn(\"weighted_rating\", (col(\"rating\") * col(\"similarity\")).cast(FloatType()))\n",
    "    numerator = user_ratings.agg(spark_sum(\"weighted_rating\")).collect()[0][0]\n",
    "    denominator = user_ratings.agg(spark_sum(\"similarity\")).collect()[0][0]\n",
    "    \n",
    "    if denominator != 0:\n",
    "        predicted_rating = numerator / denominator\n",
    "    else:\n",
    "        predicted_rating = None\n",
    "    \n",
    "    return predicted_rating\n",
    "\n",
    "# Calling Function to Predict Rating for User Id = 5 with Movie Id = 1\n",
    "predicted_rating = predict_user_rating(user_id=5, movie_id=1, smDF=df_user_movie_rating)\n",
    "print(\"Predicted Rating for User Id = 5 with Movie Id = 1 is: \", predicted_rating)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a look at the slides (at page 31), you can indeed see that we were able to predict the ratings exactly as we did in the slides. This is a good sign that our model is working as expected.\n",
    "\n",
    "<img src=\"./img/pearsonPrediction.png\" width=\"300\">\n",
    "\n",
    "**However**, being able to predict one rating is not enough. In reality, our test set would consist of multiple movies to predict. We can't just call this method one-at-a-time because it's simply not scalable. \n",
    "\n",
    "This is where we started to get a lot of issues with our logic! By the way our method was implemented, we can't register this method as a udf because we are passing a Dataframe as parameter. After trying a LOT of different ways to make it work in order to be able to register it, we decided to move on with another techniques to be able to predict multiple ratings in parallel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt to predict multiple ratings with another techniques...\n",
    "This is a tentative to predict multiple ratings in parallel (**Spoiler Alert**: it did not work...) We tried fixing it with these following steps:\n",
    "1. Take our Similarity Matrix and Pivot it so that we have a matrix containing all the similarity combinations (# of Movies x # of Movies)\n",
    "2. From there, we would have a sparse matrix where we would have to fill the missing values by multiplying all the similarity of a row with the rating of the users. We would then be able to use this matrix to predict the ratings for each user (in theory)\n",
    "\n",
    "However, in reality, it wasn't scalable... Our code was working and producing the right results, but due to the multitude of operations, it was simply too slow for 100k ratings.\n",
    "\n",
    "In the end, it was actually a good thing to move away from this technique because of these issues:\n",
    "- It was not scalable\n",
    "- It's not efficient to predict EVERY values (including the ones that are not in the test set)\n",
    "- We can't decide the number of items most similar to x. We need to multiply all of them, which is not efficient. We need to find a good in-between N-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|movieId|1                   |2                   |3                  |4                   |5                   |6                   |\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|1      |0.0                 |-0.17854212213729673|0.41403933560541256|-0.10245014273309601|-0.30895719032666236|0.5870395085642741  |\n",
      "|2      |-0.17854212213729673|0.0                 |-0.5262348115842176|0.46800784077976626 |0.39891071573694176 |-0.3064397582621859 |\n",
      "|3      |0.41403933560541256 |-0.5262348115842176 |0.0                |-0.6239806502223061 |-0.2842676218074806 |0.5063696835418333  |\n",
      "|4      |-0.10245014273309601|0.46800784077976626 |-0.6239806502223061|0.0                 |0.45873490213598356 |-0.2353393621658208 |\n",
      "|5      |-0.30895719032666236|0.39891071573694176 |-0.2842676218074806|0.45873490213598356 |0.0                 |-0.21591675854376524|\n",
      "|6      |0.5870395085642741  |-0.3064397582621859 |0.5063696835418333 |-0.2353393621658208 |-0.21591675854376524|0.0                 |\n",
      "+-------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+------+-------+-------------------+\n",
      "|userId|movieId|     recommendation|\n",
      "+------+-------+-------------------+\n",
      "|    12|      1| -5.060237418793948|\n",
      "|    10|      1|  2.152689119970842|\n",
      "|     1|      1| 3.4420263248787166|\n",
      "|     3|      1|-1.8904545334861316|\n",
      "|     6|      1|-1.5029681585094823|\n",
      "|     9|      1|  4.028311735534671|\n",
      "|     7|      1| 1.2841444382906808|\n",
      "|    11|      1|  8.311247056713844|\n",
      "|     8|      1| 1.8589730306218548|\n",
      "|     5|      1| 2.0458710675813796|\n",
      "+------+-------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot the similarity matrix to get a matrix with movieId as rows and movieId_1 as columns\n",
    "similarity_matrix = similarity_matrix.groupBy(\"movieId\").pivot(\"movieId_1\").agg({\"similarity\": \"first\"}).fillna(0)\n",
    "similarity_matrix = similarity_matrix.sort(\"movieId\")\n",
    "similarity_matrix.select(similarity_matrix.columns[:10]).show(truncate=False)\n",
    "\n",
    "# Build Vector Columns From Similarity Matrix\n",
    "assembler = VectorAssembler(inputCols=similarity_matrix.columns[1:], outputCol=\"features\")\n",
    "similarity_matrix = assembler.transform(similarity_matrix).select('movieId', 'features')\n",
    "\n",
    "# Build Vector Columns From User-Movie-Rating Matrix\n",
    "assembler = VectorAssembler(inputCols=df2.columns[1:], outputCol=\"features\")\n",
    "df2 = assembler.transform(df2).select('userId', 'features')\n",
    "\n",
    "# Compute the recommendation matrix (expensive...)\n",
    "recommendation_matrix = similarity_matrix.alias(\"a\").crossJoin(df2.alias(\"b\")) \\\n",
    "    .selectExpr(\"a.movieId as movieId\", \"b.userId as userId\",\n",
    "                \"dot_product_divided_by_sum(a.features, b.features) as recommendation\").select(\"userId\", \"movieId\", \"recommendation\")\n",
    "\n",
    "recommendation_matrix.show(10)\n",
    "\n",
    "spark.stop() # Close Spark Session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Now What?\n",
    "As of now, we are not able to predict multiple values effienctly other than using a for loop. For this reason, we need to look at another techniques, otherwise we won't be able to evaluate our model... More details will follow in the next notebook!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
