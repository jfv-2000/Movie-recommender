{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: add some markdown..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries/packages\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import  Vectors\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum, count as spark_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 22:06:19 WARN Utils: Your hostname, Martin-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.53 instead (on interface en0)\n",
      "23/03/24 22:06:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 22:06:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.co_sym(x, y)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Initialization/Setup\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"recommenderTest\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\") \\\n",
    "    .config(\"spark.sql.pivotMaxValues\", \"20000\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"FATAL\")\n",
    "\n",
    "\n",
    "# UDFs Initialization/Setup\n",
    "\n",
    "# To do: remove this function in some future versions\n",
    "def pearson_average(v):\n",
    "    sum_nonzero = sum(v)\n",
    "    count_nonzero = len([e for e in v if e != 0])\n",
    "    mean = sum_nonzero / count_nonzero\n",
    "    v2 = [e - mean if e != 0 else 0 for e in v] # now subtract the mean from each non zero element\n",
    "    return Vectors.dense(v2)\n",
    "\n",
    "def co_sym (x, y):\n",
    "    pearson1 = pearson_average(x)\n",
    "    pearson2 = pearson_average(y)\n",
    "    return float(pearson1.dot(pearson2)/(Vectors.norm(pearson1,2)*Vectors.norm(pearson2,2)))\n",
    "\n",
    "dot_udf = udf(co_sym, DoubleType())\n",
    "spark.udf.register(\"dot_udf\", dot_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Data Loading/Preparation\n",
    "\n",
    "# File FORMAT: userId, movieId, rating, timestamp\n",
    "df = spark.read.csv(\"data/ratings_tiny.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "|movieId|movieId_1|similarity|\n",
      "+-------+---------+----------+\n",
      "|      4|        3|-0.6239...|\n",
      "|      4|        6|-0.2353...|\n",
      "|      4|        5|0.45873...|\n",
      "|      4|        1|-0.1024...|\n",
      "|      4|        2|0.46800...|\n",
      "|      3|        4|-0.6239...|\n",
      "|      3|        6|0.50636...|\n",
      "|      3|        5|-0.2842...|\n",
      "|      3|        1|0.41403...|\n",
      "|      3|        2|-0.5262...|\n",
      "+-------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Modeling\n",
    "\n",
    "df_user_movie_rating = df # DF to be used for User-Movie-Rating Matrix (will be used later on)\n",
    "\n",
    "# -- Building Similarity Matrix --\n",
    "df = df.groupBy(\"movieId\").pivot(\"userId\").agg({\"rating\": \"first\"}).fillna(0)\n",
    "#df = df.sort(\"movieId\")\n",
    "\n",
    "# Build Vector Columns from DF Matrix\n",
    "assembler = VectorAssembler(inputCols=df.columns[1:], outputCol=\"features\")\n",
    "df_vector = assembler.transform(df).select('movieId', 'features')\n",
    "df_vector = df_vector.repartition(10)\n",
    "\n",
    "# Compute dot product of normalized vectors to fill data into Similarity Matrix\n",
    "similarity_matrix = df_vector.alias(\"a\").crossJoin(df_vector.alias(\"b\")) \\\n",
    "    .where(\"a.movieId != b.movieId\") \\\n",
    "    .selectExpr(\"a.movieId as movieId\", \"b.movieId as movieId_1\",\n",
    "                \"dot_udf(a.features, b.features) as similarity\")\n",
    "similarity_matrix.show(10, 10)\n",
    "\n",
    "# Build User-Movie-Rating Matrix (where for each user, we have all the movies combinations with the similarity values)\n",
    "df_user_movie_rating = df_user_movie_rating.join(similarity_matrix, df_user_movie_rating.movieId == similarity_matrix.movieId, how='left').drop(similarity_matrix.movieId)\n",
    "df_user_movie_rating = df_user_movie_rating.withColumnRenamed(\"similarity.movieId\", \"movie2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+--------------------+\n",
      "|movieId|userId|rating|movieId_1|          similarity|\n",
      "+-------+------+------+---------+--------------------+\n",
      "|      1|     1|     1|        2|-0.17854212213729673|\n",
      "|      1|     1|     1|        5|-0.30895719032666236|\n",
      "|      1|     1|     1|        6|  0.5870395085642741|\n",
      "|      1|     1|     1|        3| 0.41403933560541256|\n",
      "|      1|     1|     1|        4|-0.10245014273309601|\n",
      "|      1|     3|     3|        2|-0.17854212213729673|\n",
      "|      1|     3|     3|        5|-0.30895719032666236|\n",
      "|      1|     3|     3|        6|  0.5870395085642741|\n",
      "|      1|     3|     3|        3| 0.41403933560541256|\n",
      "|      1|     3|     3|        4|-0.10245014273309601|\n",
      "+-------+------+------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_rating1 2.5864068884261053\n",
      "predicted_rating2 1.720103231368863\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpredicted_rating1\u001b[39m\u001b[39m\"\u001b[39m, predicted_rating1)\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpredicted_rating2\u001b[39m\u001b[39m\"\u001b[39m, predicted_rating2)\n\u001b[0;32m---> 30\u001b[0m spark\u001b[39m.\u001b[39;49mclose() \u001b[39m# Close Spark Session\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "# Data Prediction\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import array, col, sort_array\n",
    "\n",
    "def predict_user_rating(user_id, movie_id, similarity_matrix):\n",
    "    # Filter similarity matrix to include only ratings for the given user and similar movies\n",
    "    user_ratings = similarity_matrix.filter((col(\"userId\") == user_id) & (col(\"movieId_1\") == movie_id))\n",
    "    # Sort the ratings by similarity in descending order and select the top 2 most similar movies\n",
    "    user_ratings = user_ratings.sort(col(\"similarity\").desc()).limit(2)\n",
    "    # Calculate the predicted rating by computing a weighted average of the user's ratings for similar movies\n",
    "    user_ratings = user_ratings.withColumn(\"weighted_rating\", (col(\"rating\") * col(\"similarity\")).cast(FloatType()))\n",
    "    numerator = user_ratings.agg(sum(\"weighted_rating\")).collect()[0][0]\n",
    "    denominator = user_ratings.agg(sum(\"similarity\")).collect()[0][0]\n",
    "    \n",
    "    if denominator != 0:\n",
    "        predicted_rating = numerator / denominator\n",
    "    else:\n",
    "        predicted_rating = None\n",
    "    return predicted_rating\n",
    "\n",
    "\n",
    "df_user_movie_rating.show(10)\n",
    "\n",
    "predicted_rating1 = predict_user_rating(user_id=5, movie_id=1, similarity_matrix=df_user_movie_rating)\n",
    "predicted_rating2 = predict_user_rating(user_id=5, movie_id=3, similarity_matrix=df_user_movie_rating)\n",
    "print(\"predicted_rating1\", predicted_rating1)\n",
    "print(\"predicted_rating2\", predicted_rating2)\n",
    "\n",
    "spark.close() # Close Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ==============Prediction V1 (OUR FOCUS RIGHT NOW)==============\n",
    "\n",
    "# Multiply the similarity matrix by the user-movie rating matrix...\n",
    "# TODO: This is not working... need to figure out why...\n",
    "# Useful Tutorial: https://www.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/\n",
    "\n",
    "# --------------------------- CODE (BELOW) ---------------------------\n",
    "\n",
    "# from pyspark.ml.feature import PCA\n",
    "# from pyspark.sql.functions import sum\n",
    "\n",
    "# # perform the dot product using PCA\n",
    "# dot_product_df = PCA(k=1, inputCol=\"features\", outputCol=\"dot_product\").fit(df_vector).transform(similarity_matrix)\n",
    "\n",
    "# # calculate the weighted average using sum\n",
    "# weighted_average = similarity_matrix.select(sum(abs(similarity_matrix.columns))).collect()[0][0]\n",
    "\n",
    "# # divide the dot product by the weighted average\n",
    "# result = dot_product_df.select(\"dot_product\").rdd.map(lambda x: x[0] / weighted_average).collect()\n",
    "# result.show()\n",
    "\n",
    "\n",
    "\n",
    "#-------\n",
    "## Pivot the similarity matrix\n",
    "# similarity_matrix = similarity_matrix.groupBy(\"movieId\").pivot(\"movieId_1\").agg({\"similarity\": \"first\"}).fillna(0)\n",
    "# similarity_matrix = similarity_matrix.sort(\"movieId\")\n",
    "# similarity_matrix.select(similarity_matrix.columns[:10]).show(truncate=False)\n",
    "# convert columns to vectors\n",
    "# assembler = VectorAssembler(inputCols=similarity_matrix.columns[1:], outputCol=\"features\")\n",
    "# similarity_matrix = assembler.transform(similarity_matrix).select('movieId', 'features')\n",
    "# print('similarity matrix done')\n",
    "\n",
    "# # convert df2 columns to vectors\n",
    "# assembler = VectorAssembler(inputCols=df2.columns[1:], outputCol=\"features\")\n",
    "# df2 = assembler.transform(df2).select('userId', 'features')\n",
    "\n",
    "# print('starting recommendation matrix')\n",
    "# # Compute the recommendation matrix\n",
    "# recommendation_matrix = similarity_matrix.alias(\"a\").crossJoin(df2.alias(\"b\")) \\\n",
    "#     .selectExpr(\"a.movieId as movieId\", \"b.userId as userId\",\n",
    "#                 \"dot_product_divided_by_sum(a.features, b.features) as recommendation\").select(\"userId\", \"movieId\", \"recommendation\")\n",
    "\n",
    "# recommendation_matrix.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
