{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:15 WARN Utils: Your hostname, Martin-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.53 instead (on interface en0)\n",
      "23/03/23 22:13:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:30 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/03/23 22:13:30 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/03/23 22:13:30 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                        (0 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/23 22:13:31 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|     1|[{1172, 3.4955099...|\n",
      "|     3|[{27773, 4.371879...|\n",
      "|     5|[{1948, 4.533952}...|\n",
      "|     6|[{83318, 4.673204...|\n",
      "|    12|[{3879, 4.9009957...|\n",
      "|    13|[{88125, 4.13369}...|\n",
      "|    15|[{4302, 4.893448}...|\n",
      "|    16|[{318, 4.9723654}...|\n",
      "|    19|[{83411, 5.161693...|\n",
      "|    20|[{51471, 4.917141...|\n",
      "|    22|[{67504, 4.292516...|\n",
      "|    26|[{65514, 4.481471...|\n",
      "|    27|[{318, 4.6969137}...|\n",
      "|    28|[{1172, 5.132921}...|\n",
      "|    31|[{83318, 4.783527...|\n",
      "|    34|[{83318, 4.953843...|\n",
      "|    37|[{68073, 5.136165...|\n",
      "|    40|[{67504, 5.185649...|\n",
      "|    41|[{83359, 4.917856...|\n",
      "|    43|[{54328, 4.326411...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Best hyperparameters:\n",
      "  maxIter: 15\n",
      "  regParam: 0.10\n",
      "  rank: 50\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Item-Item Recommender System\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the data as a Spark DataFrame\n",
    "csv_file_path = \"data/ratings_small.csv\"\n",
    "data = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Split the data into a training set and a testing set\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the ALS algorithm for collaborative filtering\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "# Set up the hyperparameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter, [5, 10, 15]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(als.rank, [10, 20, 50]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator for RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Set up cross-validation\n",
    "cross_validator = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Perform cross-validation and get the best model\n",
    "cv_model = cross_validator.fit(train_data)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model by calculating the RMSE (Root Mean Squared Error)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {:.4f}\".format(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "user_recs = best_model.recommendForAllUsers(10)\n",
    "user_recs.show()\n",
    "\n",
    "# After fitting the cross-validator and obtaining the best model\n",
    "print(\"Best hyperparameters:\")\n",
    "print(\"  maxIter: {}\".format(best_model._java_obj.parent().getMaxIter()))\n",
    "print(\"  regParam: {:.2f}\".format(best_model._java_obj.parent().getRegParam()))\n",
    "print(\"  rank: {}\".format(best_model.rank))\n",
    "\n",
    "# Best hyperparameters:\n",
    "#   maxIter: 15\n",
    "#   regParam: 0.10\n",
    "#   rank: 50\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
